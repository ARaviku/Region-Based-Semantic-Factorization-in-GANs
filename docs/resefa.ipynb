{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"resefa.ipynb","provenance":[],"collapsed_sections":["qrzMM9vnpkUz"],"authorship_tag":"ABX9TyN9ivjyZDRL+XpkK2kNLIkA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Fetch Codebase and Models\n"],"metadata":{"id":"lJ8iDbl_pmCK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOY_toBboLws"},"outputs":[],"source":["import os\n","\n","os.chdir('/content')\n","CODE_DIR = 'resefa'\n","if not os.path.exists(CODE_DIR):\n","  !git clone https://github.com/zhujiapeng/resefa.git $CODE_DIR\n","os.chdir(f'./{CODE_DIR}')\n","MODEL_DIR = 'models/pretrain'\n","os.makedirs(MODEL_DIR, exist_ok=True)\n","\n","!wget wget https://hkustconnect-my.sharepoint.com/:u:/g/personal/jzhubt_connect_ust_hk/ETYVen9KXGlAia2gH6pcZswB9Lw-21vWrE75OACvG2SBow\\?e\\=SCGqg0\\&download=1 -O $MODEL_DIR/stylegan2-ffhq-config-f-1024x1024.pth  --quiet \n","!nvidia-smi\n","!du -sh $MODEL_DIR/*"]},{"cell_type":"markdown","source":["# Define Utility Functions"],"metadata":{"id":"qrzMM9vnpkUz"}},{"cell_type":"code","source":["# python 3.7\n","\"\"\"Demo.\"\"\"\n","import io\n","import cv2\n","import warnings\n","import numpy as np\n","import torch\n","from PIL import Image\n","import IPython.display\n","from models import build_model\n","\n","warnings.filterwarnings(action='ignore', category=UserWarning)\n","\n","def postprocess_image(image, min_val=-1.0, max_val=1.0):\n","    \"\"\"Post-processes image to pixel range [0, 255] with dtype `uint8`.\n","\n","    This function is particularly used to handle the results produced by deep\n","    models.\n","\n","    NOTE: The input image is assumed to be with format `NCHW`, and the returned\n","    image will always be with format `NHWC`.\n","\n","    Args:\n","        image: The input image for post-processing.\n","        min_val: Expected minimum value of the input image.\n","        max_val: Expected maximum value of the input image.\n","\n","    Returns:\n","        The post-processed image.\n","    \"\"\"\n","    assert isinstance(image, np.ndarray)\n","\n","    image = image.astype(np.float64)\n","    image = (image - min_val) / (max_val - min_val) * 255\n","    image = np.clip(image + 0.5, 0, 255).astype(np.uint8)\n","\n","    assert image.ndim == 4 and image.shape[1] in [1, 3, 4]\n","    return image.transpose(0, 2, 3, 1)\n","\n","\n","def to_numpy(data):\n","    \"\"\"Converts the input data to `numpy.ndarray`.\"\"\"\n","    if isinstance(data, (int, float)):\n","        return np.array(data)\n","    if isinstance(data, np.ndarray):\n","        return data\n","    if isinstance(data, torch.Tensor):\n","        return data.detach().cpu().numpy()\n","    raise TypeError(f'Not supported data type `{type(data)}` for '\n","                    f'converting to `numpy.ndarray`!')\n","\n","\n","def linear_interpolate(latent_code,\n","                       boundary,\n","                       layer_index=None,\n","                       start_distance=-10.0,\n","                       end_distance=10.0,\n","                       steps=7):\n","    \"\"\"Interpolate between the latent code and boundary.\"\"\"\n","    assert (len(latent_code.shape) == 3 and len(boundary.shape) == 3 and\n","            latent_code.shape[0] == 1 and boundary.shape[0] == 1 and\n","            latent_code.shape[1] == boundary.shape[1])\n","    linspace = np.linspace(start_distance, end_distance, steps)\n","    linspace = linspace.reshape([-1, 1, 1]).astype(np.float32)\n","    inter_code = linspace * boundary\n","    is_manipulatable = np.zeros(inter_code.shape, dtype=bool)\n","    is_manipulatable[:, layer_index, :] = True\n","    mani_code = np.where(is_manipulatable, latent_code+inter_code, latent_code)\n","    return mani_code\n","\n","\n","def imshow(images, col, viz_size=256):\n","  \"\"\"Shows images in one figure.\"\"\"\n","  num, height, width, channels = images.shape\n","  assert num % col == 0\n","  row = num // col\n","\n","  fused_image = np.zeros((viz_size*row, viz_size*col, channels), dtype=np.uint8)\n","\n","  for idx, image in enumerate(images):\n","    i, j = divmod(idx, col)\n","    y = i * viz_size\n","    x = j * viz_size\n","    if height != viz_size or width != viz_size:\n","      image = cv2.resize(image, (viz_size, viz_size))\n","    fused_image[y:y + viz_size, x:x + viz_size] = image\n","\n","  fused_image = np.asarray(fused_image, dtype=np.uint8)\n","  data = io.BytesIO()\n","  if channels == 4:\n","    Image.fromarray(fused_image).save(data, 'png')\n","  elif channels == 3:\n","    Image.fromarray(fused_image).save(data, 'jpeg')\n","  else:\n","    raise ValueError('Image channel error')\n","  im_data = data.getvalue()\n","  disp = IPython.display.display(IPython.display.Image(im_data))\n","  return disp"],"metadata":{"id":"1cfMz8wQqwIZ","executionInfo":{"status":"ok","timestamp":1656176890438,"user_tz":-480,"elapsed":4279,"user":{"displayName":"Jiapeng Zhu","userId":"01756019998161234762"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Manipulation with provided boundaries"],"metadata":{"id":"yhOiwhtspxKJ"}},{"cell_type":"markdown","source":["Bulid generator and load boundaries"],"metadata":{"id":"M2v55XVoDjPH"}},{"cell_type":"code","source":["print('Building generator')\n","checkpoint_path=f'{MODEL_DIR}/stylegan2-ffhq-config-f-1024x1024.pth'\n","config = dict(model_type='StyleGAN2Generator',\n","              resolution=1024,\n","              w_dim=512,\n","              fmaps_base=int(1 * (32 << 10)),\n","              fmaps_max=512,)\n","generator = build_model(**config)\n","print(f'Loading checkpoint from `{checkpoint_path}` ...')\n","checkpoint = torch.load(checkpoint_path, map_location='cpu')['models']\n","if 'generator_smooth' in checkpoint:\n","    generator.load_state_dict(checkpoint['generator_smooth'])\n","else:\n","    generator.load_state_dict(checkpoint['generator'])\n","generator = generator.eval().cuda()\n","print('Finish loading checkpoint.')\n","\n","print('Loading boundaries')\n","ATTRS = ['eyebrows', 'eyesize', 'gaze_direction', 'nose_length', 'mouth', 'lipstick']\n","boundaries = {}\n","for attr in ATTRS:\n","  boundary_path = os.path.join(f'directions/ffhq/stylegan2/{attr}.npy')\n","  boundary = np.load(boundary_path)\n","  boundaries[attr] = boundary\n","print('Generator and boundaries are ready.')"],"metadata":{"id":"ToU0g2om-G5W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sampe latent codes and generate original images"],"metadata":{"id":"buQXOA2LDwSS"}},{"cell_type":"code","source":["print('Sampling latent codes with given seed.')\n","num_of_image = 6 #@param {type:\"slider\", min:1, max:8, step:1}\n","seed = 7464 #@param {type:\"slider\", min:0, max:10000, step:1}\n","trunc_psi = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","trunc_layers = 8\n","np.random.seed(seed)\n","latent_z = np.random.randn(num_of_image, generator.z_dim)\n","latent_z = torch.from_numpy(latent_z.astype(np.float32))\n","latent_z = latent_z.cuda()\n","wp = generator.mapping(latent_z, None)['wp']\n","if trunc_psi < 1.0:\n","    w_avg = generator.w_avg\n","    w_avg = w_avg.reshape(1, -1, generator.w_dim)[:, :trunc_layers]\n","    wp[:, :trunc_layers] = w_avg.lerp(wp[:, :trunc_layers], trunc_psi)\n","with torch.no_grad():\n","    images_ori = generator.synthesis(wp)['image']\n","images_ori = postprocess_image(to_numpy(images_ori))\n","print('Original images are shown as belows.')\n","imshow(images_ori, col=images_ori.shape[0])\n","latent_wp = to_numpy(wp)\n"],"metadata":{"id":"B1KypKUeD0D9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Manipulate"],"metadata":{"id":"69NnBUdfFF0Q"}},{"cell_type":"code","source":["#@title { display-mode: \"form\", run: \"auto\" }\n","\n","eyebrows = 0 #@param {type:\"slider\", min:-12.0, max:12.0, step:2}\n","eyesize = 0 #@param {type:\"slider\", min:-12.0, max:12.0, step:2}\n","gaze_direction = 0 #@param {type:\"slider\", min:-12.0, max:12.0, step:2}\n","nose_length = 0 #@param {type:\"slider\", min:-12.0, max:12.0, step:2}\n","mouth = 0 #@param {type:\"slider\", min:-12.0, max:12.0, step:2}\n","lipstick = 0 #@param {type:\"slider\", min:-12.0, max:12.0, step:2}\n","\n","new_codes = latent_wp.copy()\n","for attr_name in ATTRS:\n","  if attr_name in ['eyebrows', 'lipstick']:\n","      layers_idx = [8,9,10,11]\n","  else:\n","      layers_idx = [4,5,6,7]\n","  step =  eval(attr_name)\n","  direction = boundaries[attr_name]\n","  direction = np.tile(direction, [1, generator.num_layers, 1])\n","  new_codes[:, layers_idx, :] += direction[:, layers_idx, :] * step\n","new_codes = torch.from_numpy(new_codes.astype(np.float32)).cuda()\n","with torch.no_grad():\n","    images_mani = generator.synthesis(new_codes)['image']\n","images_mani = postprocess_image(to_numpy(images_mani))\n","imshow(images_mani, col=images_mani.shape[0])"],"metadata":{"id":"gP8rTVgXqvaD"},"execution_count":null,"outputs":[]}]}